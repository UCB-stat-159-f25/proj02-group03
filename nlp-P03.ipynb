{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8315241c-db67-44e8-a148-b1c5e3600465",
   "metadata": {},
   "source": [
    "# Project 2: Reproducibility in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f2d23-06ce-46cc-843b-1a4ed7dbc762",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)\n",
    "\n",
    "In this notebook, we will compare two methods for creating topic models of the speeches we've been analyzing: Latent Dirichlet allocation (LDA) and BERTopic. To begin, we need to import our requisite packages.\n",
    "\n",
    "### Imports\n",
    "In **Part 2,** we downloaded spaCy's English language text processing model `en_core_web_sm` into our environment. If, for whatever reason, you have reached this point without downloading it, please do so now. While having your `sotu` environment activated, run the following:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32f542-5b61-4068-a52f-cf3cac7c7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b95b5-bdc5-469e-adff-68caf419f54b",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf4170-17f9-4a0a-82f9-b413fd1fe245",
   "metadata": {},
   "outputs": [],
   "source": [
    "sou = pd.read_csv(\"data/SOTU.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed4e21-7bf1-4ddd-aa70-df8d17ec2d26",
   "metadata": {},
   "source": [
    "### LDA\n",
    "LDA's \"bag-of-words\" approach is much more sensitive to text preprocessing, so the function below uses spaCy to tokenize the text; cut tokens down to their semantic \"root\" (e.g., \"runs\" and \"running\" become \"run\"); and remove stop words, punctiation, and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed5a3d-7d36-4eb5-b94c-96ab9379af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    doc = nlp(text) \n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b7816-a1fd-4c83-97b2-6633cc1e860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts - note this takes ~ 5 minutes to run\n",
    "processed_docs = sou['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b1a22-f332-413d-98e1-7b3891f8b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\n",
    "dictionary = Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5) # remove both highly common and rare tokens\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs] # build a corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56a856-013b-43f2-ae4a-5d5bd1f1d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model with 18 topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, passes=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29e99b-1189-4182-9258-985c65a42bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- State of the Union LDA Topics ---\") \n",
    "for idx, topic in lda_model.print_topics(-1): \n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f18b5-59ac-49ca-a2ac-806f109df8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topic distribution for the first speech\n",
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06c589-a4c0-4616-9033-d4503040e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a visualization using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "ldavis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(ldavis, 'outputs/lda_visualization.html')\n",
    "ldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a0cdf-fb6a-42b8-a498-0f7c8c6a041b",
   "metadata": {},
   "source": [
    "### BERTopic\n",
    "BERTopic is better at handling semantic richness—or the messiness of natural language—so to start we don't need to any text preprocessing. All we do is list each speech as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23d2b7-b733-4bdf-a485-43a10b8e41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sou['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6b096-a8b1-4e6a-a21d-769c90b918d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model - this takes about 30 seconds\n",
    "topic_model = BERTopic(min_topic_size=3)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# remove stop words from the topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3), min_df=10)\n",
    "topic_model.update_topics(docs, vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f569b-cb23-4eb4-92f0-08d6914d058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the top 10 words for each topic\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f2b32-2f02-40c4-b89f-9f0cf3baff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the topic distribution for the first speech\n",
    "topic_distr, _ = topic_model.approximate_distribution(docs)\n",
    "first_speech_distr = topic_model.visualize_distribution(topic_distr[0])\n",
    "first_speech_distr.write_html(\"outputs/first_speech_distr.html\")\n",
    "first_speech_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6b7fa-1d36-4c7d-b95b-495b88c7b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to visualize the topics\n",
    "topic_model.visualize_topics()\n",
    "bertopicvis = topic_model.visualize_topics()\n",
    "bertopicvis.write_html(\"outputs/bertopicvis.html\")\n",
    "bertopicvis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython - sotu",
   "language": "python",
   "name": "sotu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
